{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Time series (part 1)\n",
    "---\n",
    "**Author(s):** Quentin Yeche, Kenji Ose, Dino Ienco - [UMR TETIS](https://umr-tetis.fr) / [INRAE](https://www.inrae.fr/)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "In this notebook we will introduce the time dimension into our exploration of data cubes. We will also cover handling masks, and specifically cloud cover masks, as well as the extraction of spectral signatures\n",
    "\n",
    "\n",
    "### Outline\n",
    "\n",
    "In [section 3](#3-getting-a-sentinel-2-image-time-series) we will be retrieving a Sentinel-2 time series from a STAC catalog.\n",
    "\n",
    "In [section 4](#4-operations-on-the-time-dimension) we will learn about common operations on the time dimension\n",
    "\n",
    "Finally in [section 5](#5-dealing-with-duplicate-dates) we will see how to deal with duplicate dates in a time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Library imports\n",
    "\n",
    "As usual, we import all the required Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is only useful if you're using an environment like Google Collab or\n",
    "# Microsoft Planetary Computer's servers\n",
    "def test_import_or_install(imports):\n",
    "  import importlib\n",
    "  restart_needed = False\n",
    "  for import_name, pip_name in imports:\n",
    "    try:\n",
    "      importlib.import_module(import_name, package=None)\n",
    "    except ModuleNotFoundError:\n",
    "      if not restart_needed:\n",
    "          restart_needed= True\n",
    "          print('\\033[91m' + (\"ONE OR MORE LIBRARIES HAVE TO BE INSTALLED, \"\n",
    "          \"PLEASE RESTART THE NOTEBOOK AFTER THIS CELL FINISHES EXECUTING \"\n",
    "          \"TO ENSURE PROPER FUNCTIONALITY\") + \"\\x1b[0m\")\n",
    "      %pip install {pip_name}\n",
    "  if restart_needed:\n",
    "    raise ImportError('\\033[91m' + (\"LIBRARIES HAVE BEEN INSTALLED. \"\n",
    "          \"PLEASE RESTART THE NOTEBOOK NOW \") + \"\\x1b[0m\")\n",
    "  \n",
    "imports = [('pystac_client', 'pystac-client'),\n",
    "           ('planetary_computer', 'planetary-computer'),\n",
    "           ('stackstac', 'stackstac'),\n",
    "           ]\n",
    "\n",
    "test_import_or_install(imports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T10:45:07.736230Z",
     "start_time": "2024-10-25T10:45:07.731272Z"
    }
   },
   "outputs": [],
   "source": [
    "# STAC access\n",
    "import pystac_client\n",
    "import planetary_computer\n",
    "\n",
    "# dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# xarrays\n",
    "import xarray as xr\n",
    "\n",
    "# library for turning STAC objects into xarrays\n",
    "import stackstac\n",
    "\n",
    "# visualization\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# miscellanous\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Getting a Sentinel-2 image time series\n",
    "\n",
    "### 3.1. Request on a STAC catalog\n",
    "\n",
    "As a practical use case let's consider that we have identified the STAC Collection we're interested in (see [this notebook](01-STAC.ipynb) for a refresher), and we also have an area of interest defined as a bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T07:49:11.602821Z",
     "start_time": "2024-10-25T07:49:06.170570Z"
    }
   },
   "outputs": [],
   "source": [
    "aoi_bounds = (3.875107329166124, 43.48641456618909, 4.118824575734205, 43.71739887308995)\n",
    "\n",
    "# retrieving the relevant STAC Item\n",
    "catalog = pystac_client.Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "    modifier=planetary_computer.sign_inplace,\n",
    "    )\n",
    "\n",
    "time_range = f\"2020/2022\"\n",
    "search = catalog.search(\n",
    "    collections=['sentinel-2-l2a'],\n",
    "    datetime=time_range,\n",
    "    bbox=aoi_bounds\n",
    ")\n",
    "items = search.item_collection()\n",
    "print(f\"{len(items)} items found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, let's print out the assets description of the first item (or image):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T07:49:11.623006Z",
     "start_time": "2024-10-25T07:49:11.604711Z"
    }
   },
   "outputs": [],
   "source": [
    "item = items[0]\n",
    "assets = [asset.title for asset in item.assets.values()]\n",
    "descriptions = pd.DataFrame(assets, columns=['Description'],\n",
    "                            index=pd.Series(item.assets.keys(), name='asset_key'))\n",
    "descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style='color:red'> **IMPORTANT:** The URLs that we obtain from the STAC Catalog are not valid indefinitely. They expire after about 30 minutes. If you see an error in the notebook, it is likely because the URL that you obtained by running the first few cells has now expired.</span> If that happens you should be able to just run the notebook again from the top to get a new URL. You can get longer-lasting URLs by signing up for Planetary Computer (which is free at the time of writing this notebook). More info [here](https://planetarycomputer.microsoft.com/docs/concepts/sas/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Creating a datacube from STAC search results\n",
    "\n",
    "This process is very similar to what we did to create a DataArray in the beginning of the [previous notebook](03-Xarray.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T07:49:11.907473Z",
     "start_time": "2024-10-25T07:49:11.624375Z"
    }
   },
   "outputs": [],
   "source": [
    "bands = ['B02', 'B03', 'B04', 'B05', 'B06', 'B07', 'B08', 'B11', 'B12']\n",
    "array = stackstac.stack(\n",
    "                    items,\n",
    "                    assets = bands,\n",
    "                    resolution=10,\n",
    "                    bounds_latlon=aoi_bounds,\n",
    "                    )\n",
    "array "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in a few lines of code we have created a DataArray which data has a size in the tens of gigabytes. The idea of lazy computations and not downloading data until necessary starts to make a lot more sense with these sizes. When we also consider the fact our area of interest is only about 5% of a full Sentinel-2 tile, the ability to download only part of a tile also becomes very important. If we had to download the entire tiles before creating this array the volume of data would probably be measured in terabytes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Checking for invalid values\n",
    "\n",
    "Before moving on, it is always a good idea to check the validity of the DataArray we just created. There are two main things to check for:\n",
    " - The presence of fill values. As described in the [previous notebook](03-Xarray.ipynb), the fill value `np.nan` is used by `stackstac.stack` in order to replace missing STAC Assets.\n",
    " - The presence of NODATA pixels. In our case (Sentinel-2) pixel values of 0 are used to flag that the pixel does not contain valid data.\n",
    "\n",
    "Of course, checking for these values necessitate to download the data. Thus we can really only look at a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T07:49:25.621359Z",
     "start_time": "2024-10-25T07:49:13.149844Z"
    }
   },
   "outputs": [],
   "source": [
    "# we're only taking the first 4 dates\n",
    "sample = array.isel(time=slice(0,4))\n",
    "fill_values = sample.isnull().sum().values\n",
    "\n",
    "print(f\"Data contains {fill_values} fill value pixels\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected we see no fill values. Indeed we have no missing Asset in any of our Items. Our collection of STAC Items is homogenous, all Sentinel-2 Items contain the right Assets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Checking for NODATA\n",
    "\n",
    "There are two situations in which NODATA pixels can appear. The first is extents which do not strictly overlap. Satellite images are almost always distributed as multi-dimensional array with a rectangular shape. If the bounds do not exactly match then the image is completed with NODATA values to make up a rectangular shape.\n",
    "\n",
    "\n",
    "<figure align=\"center\">\n",
    "  <img src=\"resources/extent_nodata.png\" width=\"30%\" alt=\"STAC Item diagram\">\n",
    "  <figcaption>Sentinel-2 image with NODATA values\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "The second situation where NODATA values can appear is when some pixels are flagged as invalid despite being inside the boundaries of the area being captured by the captor. Typically the amount of such invalid pixels is very low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T18:41:35.700684Z",
     "start_time": "2024-10-24T18:41:35.697245Z"
    }
   },
   "source": [
    "---\n",
    "> **Exercise**\n",
    "1. Find the percentage of NODATA pixels with the two following methods:\n",
    "  - Using `sample` and `xr.where`\n",
    "  - Using the temporal mean of the `s2:nodata_pixel_percentage` metadata field (you can use `np.mean`).\n",
    "2. Conlude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T10:47:28.148720Z",
     "start_time": "2024-10-25T10:47:16.089931Z"
    }
   },
   "outputs": [],
   "source": [
    "# to fill\n",
    "# TOREMOVE\n",
    "\n",
    "no_data = xr.where(sample == 0, 1, 0).sum().values\n",
    "no_data_meta = np.mean(sample[\"s2:nodata_pixel_percentage\"].values)\n",
    "print(f\"{no_data/sample.size:.1%} of values are NODATA using the sample and {no_data_meta:.1f}% using metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><b>Explanation</b></summary>\n",
    "\n",
    ">We can see that almost half of the pixels in the sample are invalid. So it appears that at least some our images do not fully overlap with our area of interest.\n",
    "> The first thing we can look at is the `s2:nodata_pixel_percentage`. It gives us the percentage of pixels with NODATA in the tile for each date.\n",
    "> <span style='color:red'> **IMPORTANT:**</span> Planetary Computer provides a few metadata properties along with the data.\n",
    " For Sentinel-2 the most useful include:\n",
    " > - `s2:nodata_pixel_percentage`\n",
    " > - `s2:water_percentage`\n",
    " > - `s2:vegetation_percentage`\n",
    " > - `eo:cloud_cover`\n",
    " > - `s2:high_proba_clouds_percentage`\n",
    " > - `s2:cloud_shadow_percentage`\n",
    ">However <span style='color:red'> **these values were computed from the whole tile (an area of up to 110 x 110 km²), and they are NOT updated when only a sub-section of a tile is considered or downloaded**</span>. The values can still be used as a proxy or rough estimate without having to download the data. It is particularly useful when gathering data in order to decide what to keep or throw out. But be careful not to confuse them with the actual value which can be computed from a sub-section of a tile. For instance a Sentinel-2 tile could have a low value for `eo:cloud_cover`, indicating that there are relatively few clouds in the whole tile. But that doesn't mean that our area of interest could not contain some of these few clouds, leading to unusable data.\n",
    "---\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look more closely at the `s2:nodata_pixel_percentage` field for each date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f'{pct:.2f} %' for pct in sample['s2:nodata_pixel_percentage']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Half the dates have more than 85% of their pixels labelled as NODATA, while the others have few to none. Let's see if this behavior applies to the whole data cube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array['s2:nodata_pixel_percentage'].plot.hist()\n",
    "plt.title('nodata pixel percentage')\n",
    "plt.ylabel('image count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that we are dealing with two different types Sentinel-2 images, one of which only has a small amount of valid values. We can know more by looking at the `sat:relative_orbit` coordinate of our array. It gives us the number of the orbit during which the image was captured:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "> **Exercise**\n",
    "> * Using the `sat:relative_orbit` field, find all the existing orbit values within the datacube (Hint: `np.unique` works well with `xarray`).\n",
    "> * Find a relation between the following metadata: `s2:nodata_pixel_percentage` and `sat:relative_orbit`. A histogram may be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T12:31:48.918438Z",
     "start_time": "2024-10-25T12:31:48.915026Z"
    }
   },
   "outputs": [],
   "source": [
    "# to fill\n",
    "# ...\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images were captured with two different orbits: 8 and 108. Let's plot the same histogram as before, but discriminating between the two orbits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T07:59:22.186854Z",
     "start_time": "2024-10-25T07:59:21.901418Z"
    }
   },
   "outputs": [],
   "source": [
    "# to fill\n",
    "# ...\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suspected, one of the two orbits is mainly at fault: orbit 108. However, let's keep in mind that these values we're looking at from `s2:nodata_pixel_percentage` are for the whole satellite image which corresponds to a much bigger area than ours. But at this point there is little chance that our area happens to overlap with this small portion of valid pixels.\n",
    "\n",
    "We can check it with a similar test to what we did for the fill values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting from our sample the dates with orbit 108\n",
    "sample_108 = sample.sel(time=sample.time['sat:relative_orbit']==108)\n",
    "\n",
    "# counting the number of NODATA pixels as a percentage\n",
    "# of overall pixel count\n",
    "nodata_108 = xr.where(sample_108==0,1,0).sum().values\n",
    "print(f\"{nodata_108/sample_108.size:.1%} of pixels in the array are NODATA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our area of interest falls almost entirely outside of the area of validity for the images with orbit 108. Thus we need to simply restrict our study to orbit 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = array.sel(time=array.time['sat:relative_orbit']==8)\n",
    "array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous histograms we could see that even with orbit 8 there were a few values with higher than average amounts of invalid pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array.sel(time=array.time['s2:nodata_pixel_percentage']>5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to take a look at these dates and their data to decide whether they should be kept. Let's just decide to remove them, as this is only a handful of dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = array.sel(time=array.time['s2:nodata_pixel_percentage'] <5)\n",
    "array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 Extra: The relationship between Sentinel-2 tiles and orbits\n",
    "\n",
    "Sentinel-2 utilizes the concept of tiles. A Sentinel-2 tile delimits an area of 100x100 km². Each tile has a unique code. The tile that our area of interest falls in is the tile 31TEJ (oftentimes the T is added in front: T31TEJ). \n",
    "\n",
    "> **Note:** As an aside this code can be found in `s2:mgrs_tile` and as part of the `s2:granule_id` coordinate of our array. More information on the naming conventions used by Sentinel-2 can be found [here](https://sentinels.copernicus.eu/web/sentinel/user-guides/sentinel-2-msi/naming-convention#:~:text=The%20top%2Dlevel%20SENTINEL%2D2,separated%20by%20an%20underscore%20(_).&text=The%20Mission%20Identifier%20(MMM)%20denotes,for%20the%20SENTINEL%2D2B%20instrument.).\n",
    "\n",
    "In order to capture an image in the area delimited by the tile 31TEJ the satellite has to fly over the area along an orbit. For tile 31TEJ this orbit is orbit 8. But in order to ensure that there exists overlap between the tiles, data is actually captured with some overlap in a 110x110 km² area. \n",
    "\n",
    "Here is now the explanation for the two orbits. The images we see with orbit 108 actually correspond to the acquisition for a different, neighboring tile 31TJG. But since there is overlap between the two images, a small area near the edge of tile 31TEJ is covered by the acquisitions made with orbit 108.\n",
    "\n",
    "> **Note:** A useful interactive map of the Sentinel-2 tiling grid is available [here](https://eatlas.org.au/data/uuid/f7468d15-12be-4e3f-a246-b2882a324f59) (it can take a few seconds for the map to load in completely). It clearly shows the different tiles, their codes and the overlap between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Operations on the time dimension\n",
    "\n",
    "In the [previous notebook](02-Single-date.ipynb) we covered how to manipulate Xarrays. Now that we have introduced multiple dates in our time dimension, there are a few more tools which are useful when dealing with temporal data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Pandas dates and inexact matching\n",
    "\n",
    "There are situations in which having to match the exact values is not desirable. Date objects are usually given with a high degree of precision, and it is not practical having to always specify hours minutes and seconds in situations where the date would suffice. There are a few things which help alleviate this annoyance.\n",
    "\n",
    "The first is that Xarray uses the same logic as pandas for its date indexes. Even though the values are datetime64 objects, they can be selected with regular strings for convenience. For instance \"`2023`\" will correctly match all datetime64 objects between `2023-01-01T00:00:00` and `2023-12-31T23:59:59`, and `2023-01` will correctly match all datetime64 objects of the January 2023, etc.\n",
    "\n",
    "The other tool is the ability of the `sel` method to select inexact matches. This behavior is not limited to time dimensions, but it is particularly applicable for dates. By default, `sel` only matches exact values. However the `method` parameter can enable inexact matches:\n",
    " - `pad` or `ffill` will match the nearest value below\n",
    " - `backfill` or `bfill` will match the nearest value above\n",
    " - `nearest` will match the nearest value\n",
    "\n",
    " Another parameter of `sel` is `tolerance`. It allows to set a limit on the distance between the specified value and a possible match. For dates it can used with a `timedelta64` objet or a string like `'5D'` (5 days), `'5M3W'` (5 months 3 weeks), `'2Y15h34s'`(2 years 15 hours and 34 seconds), etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_dates = array.isel(time=slice(3,7)).time\n",
    "print(f\"Dates:\\n {few_dates.values}\")\n",
    "\n",
    "# does not match the last date as it is in February\n",
    "print(f\"\\n2020-01: \\n {few_dates.sel(time='2020-01').values}\")\n",
    "\n",
    "# matches 2020-01-18 as it is the nearest\n",
    "print(f\"\\nnearest 2020-01-18 \\n\",\n",
    "      f\"{few_dates.sel(time='2020-01-20', method='nearest').values}\")\n",
    "\n",
    "# matches 2020-01-23 as it is the nearest date after 2020-01-20\n",
    "print(f\"\\nbackfill 2020-01-18\\n\",\n",
    "      f\"{few_dates.sel(time='2020-01-20', method='backfill').values}\")\n",
    "\n",
    "# matches 2020-01-18 as it is the nearest date before 2020-01-22\n",
    "print(f\"\\npad 2020-01-22 \\n\",\n",
    "      f\"{few_dates.sel(time='2020-01-22', method='pad').values}\")\n",
    "\n",
    "# 2020-01-23 is treated as 2020-01-23T00:00:00\n",
    "# which comes before 2020-01-23T10:42:29.0240\n",
    "# thus 2020-01-18 is the correct match\n",
    "print(f\"\\npad 2023-01-22\\n\",\n",
    "      f\"{few_dates.sel(time='2020-01-23', method='pad').values}\")\n",
    "\n",
    "print(f\"\\npad 2023-01-22 with tolerance 4 days 14 hours\\n\",\n",
    "      f\"{few_dates.sel(time='2020-01-23', method='pad', tolerance='4D14h').values}\")\n",
    "\n",
    "# this one would fail to get a match and give an error\n",
    "#print(f\"\\npad 2023-01-22 with tolerance 4 days 13 hours and 5 seconds\\n\",\n",
    "#      f\"{few_dates.sel(time='2020-01-23', method='pad', tolerance='4D13h5s').values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing inherited from pandas is the `dt` accessor. It allows accessing specific components of a datetime64 object. The full list of components can be found [here](https://pandas.pydata.org/docs/user_guide/timeseries.html#time-date-components)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the data from February for all the years\n",
    "array.sel(time=(array.time.dt.month == 2))\n",
    "\n",
    "# all the data captured on the 4th day of a month\n",
    "array.sel(time=(array.time.dt.day == 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Resampling and other operations\n",
    "\n",
    "#### 4.2.1. Analyzing acquisition frequencies\n",
    "\n",
    "What follows are a few examples of operations on dates in order to extract useful information. They can be instrumental in refining a time series. It is technically possible to do more complex operations like upsampling and interpolation, but they will not be covered here.\n",
    "\n",
    "First, we create a function to format axes for dates as a fairly readable year/month string:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T08:05:34.649023Z",
     "start_time": "2024-10-25T08:05:34.644070Z"
    }
   },
   "outputs": [],
   "source": [
    "# this function is used to format axes for dates\n",
    "# as a fairly readable year/month standard\n",
    "def date_format(ax, month='%B'):\n",
    "    import matplotlib.dates as mdates\n",
    "    # adding locations and formatting for months\n",
    "    # on the x axis\n",
    "    loc = mdates.MonthLocator(bymonth=(1,4,7,10))\n",
    "    major_fmt = mdates.ConciseDateFormatter(loc,\n",
    "                formats=['%Y', month, '%d', '%H:%M', '%H:%M', '%S.%f']\n",
    "                                            )\n",
    "    ax.xaxis.set_major_formatter(major_fmt)\n",
    "    ax.xaxis.set_major_locator(loc)\n",
    "\n",
    "    # adding minor ticks for months without a label\n",
    "    ax.xaxis.set_minor_locator(mdates.MonthLocator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we calculate and plot the number of days between each acquisition to see if there are any gaps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T08:05:43.038630Z",
     "start_time": "2024-10-25T08:05:42.317132Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculating the number of days between each acquisition\n",
    "# to see if there are gaps\n",
    "fig, ax = plt.subplots(figsize=(12,1))\n",
    "ax.eventplot(array.time)\n",
    "ax.set_title(\"Chronogram of acquisitions\")\n",
    "\n",
    "date_format(ax, month='%b')\n",
    "\n",
    "# removing y axis ticks as they are meaningless\n",
    "ax.set_yticks([])\n",
    "fig.show()\n",
    "\n",
    "# plotting as a histogram\n",
    "fig, (ax1, ax2) = plt.subplots(2,1, sharex=True)\n",
    "ax1.hist(array.time.diff(dim='time').dt.round('1D').dt.days)\n",
    "ax2.hist(array.time.diff(dim='time').dt.round('1D').dt.days)\n",
    "fig.subplots_adjust(hspace=0.05)\n",
    "ax1.set_ylim(150,250)\n",
    "ax2.set_ylim(0,16)\n",
    "\n",
    "ax1.spines.bottom.set_visible(False)\n",
    "ax2.spines.top.set_visible(False)\n",
    "ax1.xaxis.tick_top()\n",
    "ax2.xaxis.tick_bottom()\n",
    "\n",
    "# adding cut-out slanted lines to signify a cut in the y axis\n",
    "d = .5\n",
    "kwargs = dict(marker=[(-1, -d), (1, d)], markersize=12,\n",
    "              linestyle=\"none\", color='k', mec='k', mew=1, clip_on=False)\n",
    "ax1.plot([0, 1], [0, 0], transform=ax1.transAxes, **kwargs)\n",
    "ax2.plot([0, 1], [1, 1], transform=ax2.transAxes, **kwargs)\n",
    "\n",
    "ax1.set_title('Number of days between two acquisitions')\n",
    "ax2.set_xlabel('Number of days')\n",
    "# supylabel allows us to name both y axes together\n",
    "fig.supylabel(\"Number of occurences\")\n",
    "plt.show()\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also can count the number of acquisitions per month:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T08:06:09.733544Z",
     "start_time": "2024-10-25T08:06:09.454173Z"
    }
   },
   "outputs": [],
   "source": [
    "# counting the number of acquisitons per month\n",
    "\n",
    "# resampling and counting for each month\n",
    "# using MS (Month Start) as we prefer using the first day\n",
    "# of the month as a reference point instead of the last\n",
    "# acquisition date of the month\n",
    "ar = array.time.resample(time='MS').count()\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "fig, ax = plt.subplots(figsize=(13,4))\n",
    "ar.plot.scatter(ax=ax, x='time')\n",
    "ax.set_ylabel('Acquisition count')\n",
    "ax.set_title(\"Number of acquisitions per month\")\n",
    "\n",
    "# setting y ticks manually to avoid non-integer values being used\n",
    "ax.set_yticks(range(ar.min().values, ar.max().values+1))\n",
    "\n",
    "date_format(ax)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2. Analyzing variations of cloud cover\n",
    "\n",
    "To finish things up, let's plot the evolution of vegetation and cloud cover percentage over time as given by `s2:vegetation_percentage` and `eo:cloud_cover` respectively. Remember, these values are actually for a bigger area than ours, but trends should usually be correct.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "# MS refers to Month Start (Month End is also possible, but rarely preferable)\n",
    "array['eo:cloud_cover'].resample(time='MS').mean().plot(ax=ax, color='slategrey')\n",
    "array['s2:vegetation_percentage'].resample(time='MS').mean().plot(ax=ax, color='forestgreen')\n",
    "ax.legend(['cloud cover', 'vegetation'])\n",
    "ax.set_ylabel('percentage')\n",
    "ax.set_title('Average cloud cover and vegetation percentage in the tile per month')\n",
    "\n",
    "date_format(ax)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally both plots are very negatively correlated since 'cloudy' and 'vegetation'  are two exclusive classes for a pixel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extra: Dealing with duplicate dates\n",
    "\n",
    "When dealing with time series it is rare but possible to encounter duplicated acquisition dates. With Sentinel-2 data there are two situations where this can happen:\n",
    " 1. The acquisition is spatially split. One packet has one part of the geometry and the other packet has the rest. Solving this requires manually looking at the extents and possibly having to fuse them.\n",
    " 2. The data was re-released after being reprocessed. The acquisition date is the same but the processing date is not the same. Unfortunately the processing date is not directly available on STAC, but it can still be found within the `s2:product_uri` property. Along other things this string contains two dates. The first is the acquisition date (the same as what can be found in the `time` dimension) and the second is the date of when the data was processed. In this situation it is typically preferable to use the reprocessed data (as there should be a good reason for the reprocessing), but sometimes the data is actually identical despite the reprocessing.\n",
    "\n",
    "What follows is one of the ways to detect and deal with duplicates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the easiest way to find duplicates is to extract the pandas index\n",
    "# from the xarray with .indexes and calling the duplicated method\n",
    "# keep=False makes it so that all all duplicates are flagged instead\n",
    "# of all duplicates but the first/last\n",
    "duplicate_indexes = array.indexes['time'].duplicated(keep=False)\n",
    "duplicates = array.sel(time=duplicate_indexes)\n",
    "duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we know that there are duplicated dates in the time series. We could look manually and see that they correspond to two different dates having each 2 acquisitions listed. In particular by looking at `s2:product_uri` we can see that the processing dates are different.\n",
    "\n",
    "However we can also do this programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_groups = duplicates.groupby('time')\n",
    "# counting for each group the number of elements in the `time`\n",
    "# dimension\n",
    "dup_groups.count().isel(x=0, y=0, band=0).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our duplicated dates with each date in its group we can check whether the data is actually different. We use the `apply` method which allows to apply a function to each group separately.\n",
    "\n",
    "What the function `lambda x: np.all(x==x[0])` does is to check whether all dates in the group have all their pixel values equal to the ones of the first date (`x[0]`) of the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_groups.apply(lambda x: np.all(x==x[0])).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case for both sets of duplicate dates the value is `True` which means that the duplicates are simply copies, thus it does not matter which copy is kept. Now we can simply call `drop_duplicates` to get unique dates in the time dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array.drop_duplicates(dim='time')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_rpg_spot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
